{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cost Sensitive 3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOsJrYBKyy2CHwVted51J3p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Data/blob/main/Cost_Sensitive_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o44k4RtPmbtP"
      },
      "source": [
        "# **Cost Sensitive Decision Trees**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9WQVsXSm5S9"
      },
      "source": [
        "In this notebook, we willl look at how to train decision trees with cost in mind"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txuTMTBDm2sx"
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from numpy import mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v347izfqnIIe"
      },
      "source": [
        "**Create an imbalanced dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc3GWUNNmLR1"
      },
      "source": [
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=3)\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4Dr4G03nPKe"
      },
      "source": [
        "# scatter plot of examples by class label \n",
        "for label, _ in counter.items():\n",
        "  row_ix = where(y == label)[0]\n",
        "  pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label)) \n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKcuZu4NpSYG"
      },
      "source": [
        "**Create a decision tree classifier and train it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxAADso4oI_M"
      },
      "source": [
        "model = DecisionTreeClassifier()\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqeulxh2oEPu"
      },
      "source": [
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.3f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAZN7TwKqXkr"
      },
      "source": [
        "# **Weighted Decision Trees**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxrR0U_WnTjX"
      },
      "source": [
        "With decision trees we can weight the splitting criteria to account for the imbalance in the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz3s22pkqfCU"
      },
      "source": [
        "Small weight = less importance, lower impact on the node purity<br>\n",
        "Large weight = more importance, greater impact on the node purity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhCQyi-WrVkm"
      },
      "source": [
        "**Define a Decision Tree Classifier model that incorporates the imbalance of the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQzlmmDCrUKQ"
      },
      "source": [
        "model = DecisionTreeClassifier(class_weight='balanced')\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nrnpSKzrn5S"
      },
      "source": [
        "**Determine model performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Reug3j2Yrsij"
      },
      "source": [
        "As you can see this method is slightly better than the non-balanced tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX511wv_rRAQ"
      },
      "source": [
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.3f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xgW7lg9sJk1"
      },
      "source": [
        "# **Grid Search Weighted Decision Tree Classifiers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Err2LDuqldlw"
      },
      "source": [
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# define grid search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv,\n",
        "scoring='roc_auc')\n",
        "# execute the grid search\n",
        "grid_result = grid.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oac-RdP2XWUQ"
      },
      "source": [
        "# report the best configuration\n",
        "print('Best: %f using %s' % (grid_result.best_score_, grid_result.best_params_)) # report all configurations\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('%f (%f) with: %r' % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuhlzRW2XWaE"
      },
      "source": [
        "# Generate and plot a synthetic imbalanced classification dataset\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=3)\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "# scatter plot of examples by class label \n",
        "for label, _ in counter.items():\n",
        "  row_ix = where(y == label)[0]\n",
        "  pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label)) \n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw1gOLceaDo1"
      },
      "source": [
        "# fit a decision tree on an imbalanced classification dataset\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=3)\n",
        "# define model\n",
        "model = DecisionTreeClassifier()\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.3f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-8ekQ0EaK9e"
      },
      "source": [
        "# decision tree with class weight on an imbalanced classification dataset\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=3)\n",
        "# define model\n",
        "model = DecisionTreeClassifier(class_weight='balanced')\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.3f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "donoHW1xaPiH"
      },
      "source": [
        "\n",
        "# grid search class weights with decision tree for imbalance classification\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=3)\n",
        "# define model\n",
        "model = DecisionTreeClassifier()\n",
        "# define grid\n",
        "balance = [{0:100,1:1}, {0:10,1:1}, {0:1,1:1}, {0:1,1:10}, {0:1,1:100}] \n",
        "param_grid = dict(class_weight=balance)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
        "# define grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n",
        "# execute the grid search\n",
        "grid_result = grid.fit(X, y)\n",
        "# report the best configuration\n",
        "print('Best: %f using %s' % (grid_result.best_score_, grid_result.best_params_)) # report all configurations\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('%f (%f) with: %r' % (mean, stdev, param)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWnbNQI8aK_H"
      },
      "source": [
        "# Generate and plot a synthetic imbalanced classification dataset\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "# scatter plot of examples by class label \n",
        "for label, _ in counter.items():\n",
        "  row_ix = where(y == label)[0]\n",
        "  pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label)) \n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyT41qrqax6S"
      },
      "source": [
        "# fit a svm on an imbalanced classification dataset\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n",
        "# define model\n",
        "model = SVC(gamma='scale')\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.3f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm52YpSha4yi"
      },
      "source": [
        "# svm with class weight on an imbalanced classification dataset\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n",
        "# define model\n",
        "model = SVC(gamma='scale', class_weight='balanced')\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.3f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvuWdeT7a9Py"
      },
      "source": [
        "\n",
        "# grid search class weights with svm for imbalance classification\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n",
        "# define model\n",
        "model = SVC(gamma='scale')\n",
        "# define grid\n",
        "balance = [{0:100,1:1}, {0:10,1:1}, {0:1,1:1}, {0:1,1:10}, {0:1,1:100}] \n",
        "param_grid = dict(class_weight=balance)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "#define grid search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n",
        "# execute the grid search\n",
        "grid_result = grid.fit(X, y)\n",
        "# report the best configuration\n",
        "print('Best: %f using %s' % (grid_result.best_score_, grid_result.best_params_)) # report all configurations\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('%f (%f) with: %r' % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10pqTpMPbPyY"
      },
      "source": [
        "# Generate and plot a synthetic imbalanced classification dataset\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=4)\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "# scatter plot of examples by class label \n",
        "for label, _ in counter.items():\n",
        "  row_ix = where(y == label)[0]\n",
        "  pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label)) \n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oFWc2hbboyj"
      },
      "source": [
        "# standard neural network on an imbalanced classification dataset\n",
        "from sklearn.datasets import make_classification \n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "# prepare train and test dataset\n",
        "def prepare_data():\n",
        "  # generate 2d classification dataset\n",
        "  X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "  n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=4)\n",
        "    # split into train and test\n",
        "  n_train = 5000\n",
        "  trainX, testX = X[:n_train, :], X[n_train:, :] \n",
        "  trainy, testy = y[:n_train], y[n_train:] \n",
        "  return trainX, trainy, testX, testy\n",
        "# define the neural network model\n",
        "def define_model(n_input):\n",
        "# define model\n",
        "  model = Sequential()\n",
        "  # define first hidden layer and visible layer \n",
        "  model.add(Dense(10, input_dim=n_input, activation='relu',kernel_initializer='he_uniform'))\n",
        "  # define output layer\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # define loss and optimizer \n",
        "  model.compile(loss='binary_crossentropy', optimizer='sgd') \n",
        "  return model\n",
        "\n",
        "# prepare dataset\n",
        "trainX, trainy, testX, testy = prepare_data()\n",
        "# define the model\n",
        "n_input = trainX.shape[1]\n",
        "model = define_model(n_input)\n",
        "# fit model\n",
        "model.fit(trainX, trainy, epochs=100, verbose=0) \n",
        "# make predictions on the test dataset\n",
        "yhat = model.predict(testX)\n",
        "# evaluate the ROC AUC of the predictions\n",
        "score = roc_auc_score(testy, yhat)\n",
        "print('ROC AUC: %.3f' % score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5RGxB7YcjiY"
      },
      "source": [
        "\n",
        "# class weighted neural network on an imbalanced classification dataset\n",
        "from sklearn.datasets import make_classification \n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "# prepare train and test dataset\n",
        "def prepare_data():\n",
        "  # generate 2d classification dataset\n",
        "  X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "  n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=4)\n",
        "    # split into train and test\n",
        "  n_train = 5000\n",
        "  trainX, testX = X[:n_train, :], X[n_train:, :] \n",
        "  trainy, testy = y[:n_train], y[n_train:] \n",
        "  return trainX, trainy, testX, testy\n",
        "\n",
        "# define the neural network model\n",
        "def define_model(n_input):\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  # define first hidden layer and visible layer \n",
        "  model.add(Dense(10, input_dim=n_input, activation='relu',\n",
        "  kernel_initializer='he_uniform'))\n",
        "  # define output layer\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # define loss and optimizer \n",
        "  model.compile(loss='binary_crossentropy', optimizer='sgd') \n",
        "  return model\n",
        "\n",
        "  # prepare dataset\n",
        "trainX, trainy, testX, testy = prepare_data()\n",
        "# get the model\n",
        "n_input = trainX.shape[1]\n",
        "model = define_model(n_input)\n",
        "# fit model\n",
        "weights = {0:1, 1:100}\n",
        "history = model.fit(trainX, trainy, class_weight=weights, epochs=100, verbose=0) # evaluate model\n",
        "yhat = model.predict(testX)\n",
        "score = roc_auc_score(testy, yhat)\n",
        "print('ROC AUC: %.3f' % score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4FQ40UXdF2K"
      },
      "source": [
        "# Generate and plot a synthetic imbalanced classification dataset\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=7)\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "# scatter plot of examples by class label \n",
        "for label, _ in counter.items():\n",
        "  row_ix = where(y == label)[0]\n",
        "  pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label)) \n",
        "  pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd4knq_MdQ8R"
      },
      "source": [
        "# fit xgboost on an imbalanced classification dataset\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=7)\n",
        "# define model\n",
        "model = XGBClassifier()\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.5f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KjpPvCEdWUt"
      },
      "source": [
        "# estimate a value for the scale_pos_weight xgboost hyperparameter\n",
        "from sklearn.datasets import make_classification \n",
        "from collections import Counter\n",
        "\n",
        "#generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=7)\n",
        "# count examples in each class\n",
        "counter = Counter(y)\n",
        "# estimate scale_pos_weight value\n",
        "estimate = counter[0] / counter[1] \n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ap3SBMIdjMc"
      },
      "source": [
        "# fit balanced xgboost on an imbalanced classification dataset\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=7)\n",
        "# define model\n",
        "model = XGBClassifier(scale_pos_weight=99)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.5f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KYBJRyIdokf"
      },
      "source": [
        "# grid search positive class weights with xgboost for imbalance classification\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=7)\n",
        "# define model\n",
        "model = XGBClassifier()\n",
        "# define grid\n",
        "weights = [1, 10, 25, 50, 75, 99, 100, 1000]\n",
        "param_grid = dict(scale_pos_weight=weights)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) # define grid search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n",
        "# execute the grid search\n",
        "grid_result = grid.fit(X, y)\n",
        "# report the best configuration\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) # report all configurations\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}