{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cost Sensitive 4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNhSKlL/7HQFZyuB3Apz4eg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Data/blob/main/Cost_Sensitive_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMsJQE-gBzFf"
      },
      "source": [
        "# **Cost Sensitive SVM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZl1G79pC9jR"
      },
      "source": [
        "SVMs work well when the dataset is balanced, but when the dataset is imbalanced performance degrades. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TjZ3EoODXrt"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD8pfwFhDVZN"
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFNueOXiDbC1"
      },
      "source": [
        "**Create an imbalanced dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1pevf1fD-Qs"
      },
      "source": [
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWnbNQI8aK_H"
      },
      "source": [
        "for label, _ in counter.items():\n",
        "  row_ix = where(y == label)[0]\n",
        "  pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label)) \n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88cEHWecESzt"
      },
      "source": [
        "**Create and train an SVM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyT41qrqax6S"
      },
      "source": [
        "model = SVC(gamma='scale')\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.3f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhPzCMS8Q1iM"
      },
      "source": [
        "**Create and train an SVM that is weight balanced to compensate for the imbalance in the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyYrmjxHRQ2f"
      },
      "source": [
        "There is a big performance improvement with the weight balanced SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm52YpSha4yi"
      },
      "source": [
        "model = SVC(gamma='scale', class_weight='balanced')\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.3f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLioJsEFR75S"
      },
      "source": [
        "**Use Grid Search to find the best weight balance for the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHnxMs9FSSgq"
      },
      "source": [
        "Note the amount of time to do a grid search compared to using the balanced weight model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvuWdeT7a9Py"
      },
      "source": [
        "model = SVC(gamma='scale')\n",
        "# define grid\n",
        "balance = [{0:100,1:1}, {0:10,1:1}, {0:1,1:1}, {0:1,1:10}, {0:1,1:100}] \n",
        "param_grid = dict(class_weight=balance)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "#define grid search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n",
        "# execute the grid search\n",
        "grid_result = grid.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TjuAsVcSFaj"
      },
      "source": [
        "# report the best configuration\n",
        "print('Best: %f using %s' % (grid_result.best_score_, grid_result.best_params_)) # report all configurations\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('%f (%f) with: %r' % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaY3DXfMShz9"
      },
      "source": [
        "**Assignment**<br>\n",
        "Change the dataset size and weights and determine if using a balanced weight model is always faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10pqTpMPbPyY"
      },
      "source": [
        "# Generate and plot a synthetic imbalanced classification dataset\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=4)\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "# scatter plot of examples by class label \n",
        "for label, _ in counter.items():\n",
        "  row_ix = where(y == label)[0]\n",
        "  pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label)) \n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oFWc2hbboyj"
      },
      "source": [
        "# standard neural network on an imbalanced classification dataset\n",
        "from sklearn.datasets import make_classification \n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "# prepare train and test dataset\n",
        "def prepare_data():\n",
        "  # generate 2d classification dataset\n",
        "  X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "  n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=4)\n",
        "    # split into train and test\n",
        "  n_train = 5000\n",
        "  trainX, testX = X[:n_train, :], X[n_train:, :] \n",
        "  trainy, testy = y[:n_train], y[n_train:] \n",
        "  return trainX, trainy, testX, testy\n",
        "# define the neural network model\n",
        "def define_model(n_input):\n",
        "# define model\n",
        "  model = Sequential()\n",
        "  # define first hidden layer and visible layer \n",
        "  model.add(Dense(10, input_dim=n_input, activation='relu',kernel_initializer='he_uniform'))\n",
        "  # define output layer\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # define loss and optimizer \n",
        "  model.compile(loss='binary_crossentropy', optimizer='sgd') \n",
        "  return model\n",
        "\n",
        "# prepare dataset\n",
        "trainX, trainy, testX, testy = prepare_data()\n",
        "# define the model\n",
        "n_input = trainX.shape[1]\n",
        "model = define_model(n_input)\n",
        "# fit model\n",
        "model.fit(trainX, trainy, epochs=100, verbose=0) \n",
        "# make predictions on the test dataset\n",
        "yhat = model.predict(testX)\n",
        "# evaluate the ROC AUC of the predictions\n",
        "score = roc_auc_score(testy, yhat)\n",
        "print('ROC AUC: %.3f' % score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5RGxB7YcjiY"
      },
      "source": [
        "\n",
        "# class weighted neural network on an imbalanced classification dataset\n",
        "from sklearn.datasets import make_classification \n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "# prepare train and test dataset\n",
        "def prepare_data():\n",
        "  # generate 2d classification dataset\n",
        "  X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "  n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=4)\n",
        "    # split into train and test\n",
        "  n_train = 5000\n",
        "  trainX, testX = X[:n_train, :], X[n_train:, :] \n",
        "  trainy, testy = y[:n_train], y[n_train:] \n",
        "  return trainX, trainy, testX, testy\n",
        "\n",
        "# define the neural network model\n",
        "def define_model(n_input):\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  # define first hidden layer and visible layer \n",
        "  model.add(Dense(10, input_dim=n_input, activation='relu',\n",
        "  kernel_initializer='he_uniform'))\n",
        "  # define output layer\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # define loss and optimizer \n",
        "  model.compile(loss='binary_crossentropy', optimizer='sgd') \n",
        "  return model\n",
        "\n",
        "  # prepare dataset\n",
        "trainX, trainy, testX, testy = prepare_data()\n",
        "# get the model\n",
        "n_input = trainX.shape[1]\n",
        "model = define_model(n_input)\n",
        "# fit model\n",
        "weights = {0:1, 1:100}\n",
        "history = model.fit(trainX, trainy, class_weight=weights, epochs=100, verbose=0) # evaluate model\n",
        "yhat = model.predict(testX)\n",
        "score = roc_auc_score(testy, yhat)\n",
        "print('ROC AUC: %.3f' % score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4FQ40UXdF2K"
      },
      "source": [
        "# Generate and plot a synthetic imbalanced classification dataset\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=7)\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "# scatter plot of examples by class label \n",
        "for label, _ in counter.items():\n",
        "  row_ix = where(y == label)[0]\n",
        "  pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label)) \n",
        "  pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd4knq_MdQ8R"
      },
      "source": [
        "# fit xgboost on an imbalanced classification dataset\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=7)\n",
        "# define model\n",
        "model = XGBClassifier()\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.5f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KjpPvCEdWUt"
      },
      "source": [
        "# estimate a value for the scale_pos_weight xgboost hyperparameter\n",
        "from sklearn.datasets import make_classification \n",
        "from collections import Counter\n",
        "\n",
        "#generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=7)\n",
        "# count examples in each class\n",
        "counter = Counter(y)\n",
        "# estimate scale_pos_weight value\n",
        "estimate = counter[0] / counter[1] \n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ap3SBMIdjMc"
      },
      "source": [
        "# fit balanced xgboost on an imbalanced classification dataset\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=7)\n",
        "# define model\n",
        "model = XGBClassifier(scale_pos_weight=99)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.5f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KYBJRyIdokf"
      },
      "source": [
        "# grid search positive class weights with xgboost for imbalance classification\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=7)\n",
        "# define model\n",
        "model = XGBClassifier()\n",
        "# define grid\n",
        "weights = [1, 10, 25, 50, 75, 99, 100, 1000]\n",
        "param_grid = dict(scale_pos_weight=weights)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) # define grid search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n",
        "# execute the grid search\n",
        "grid_result = grid.fit(X, y)\n",
        "# report the best configuration\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) # report all configurations\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}