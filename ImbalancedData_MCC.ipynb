{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNop5mSDODv9hVBVubAusuX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Data/blob/main/ImbalancedData_MCC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66kcE_OWWca5"
      },
      "outputs": [],
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -s https://github.com/cagBRT/Data.git cloned-repo\n",
        "%cd cloned-repo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import unique\n",
        "from numpy import where\n",
        "from matplotlib import pyplot\n",
        "from sklearn.datasets import make_blobs"
      ],
      "metadata": {
        "id": "PUGGtFu_ZELk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "kt8PVHQbZTfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold"
      ],
      "metadata": {
        "id": "Y1vx6hdkZziv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "t746D4E6ZXst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Precision**<br>\n",
        "A quick calculation shows that now precision is 33%, recall 25%, and the F1-score is 29% — oy vey! Our classifier is terrible at classifying cats. Let’s go on and look at the overall accuracy"
      ],
      "metadata": {
        "id": "Tr3ENqejXtBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy**<br>\n",
        "Accuracy is the proportion of samples that are correctly classified.<br>\n",
        "In total there are (TP+FP)+(FN+TN)=20+4=24 samples, and TP+TN=19 are correctly classified. The accuracy is thus a formidable 79%. But this is quite misleading, since although 90% of dogs are accurately classified, it’s only 25% for cats. If you average 90% and 25% you’ll get an average accuracy of 57.5%, which is much lower than the classifier’s accuracy of 79%. And the reason? There are many more dog samples than cat samples in our dataset. The classes are imbalanced."
      ],
      "metadata": {
        "id": "SY7DhBgiX9We"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see how the class imbalance affects the accuracy, imagine that now instead of 4 cat photos, we had 100 sets of these 4 photos for a total of 400 photos. Since we use the same classifier, 100 out of 400 of the photos will be correctly classified, and 300 will be misclassified."
      ],
      "metadata": {
        "id": "9vYReLMSYN8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"SoManyCats.png\")"
      ],
      "metadata": {
        "id": "W6XxOorsYPi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ouch! A correlation of 0.17 signifies the that predicted class and the true class are weakly correlated. And we know exactly why. Our classifier is bad at classifying cats"
      ],
      "metadata": {
        "id": "flYHedMkXYEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matthews Correlation Coefficient**\n",
        "For binary classification, there is another (and arguably more elegant) solution: treat the true class and the predicted class as two (binary) variables, and compute their correlation coefficient (in a similar way to computing correlation coefficient between any two variables). The higher the correlation between true and predicted values, the better the prediction. This is the phi-coefficient (φ), rechristened Matthews Correlation Coefficient (MCC) when applied to classifiers. Computing the MCC is not rocket science:"
      ],
      "metadata": {
        "id": "Y8XqkrXXWdCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(\"images/ConfusionMatrixMCC.png\" , width=640)"
      ],
      "metadata": {
        "id": "J7gfJFuEXDEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fact, MCC value is always between -1 and 1, with 0 meaning that the classifier is no better than a random flip of a fair coin. MCC is also perfectly symmetric, so no class is more important than the other; if you switch the positive and negative, you’ll still get the same value.\n",
        "\n",
        "MCC takes into account all four values in the confusion matrix, and a high value (close to 1) means that both classes are predicted well, even if one class is disproportionately under- (or over-) represented."
      ],
      "metadata": {
        "id": "ccov4SaDWh2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1,\n",
        "                           weights=[0.99], flip_y=0, random_state=4)\n"
      ],
      "metadata": {
        "id": "YttM91oLZg3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)"
      ],
      "metadata": {
        "id": "dWyGlZEyZmfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scatter plot of examples by class label\n",
        "for label,  _ in counter.items():\n",
        "  row_ix = where(y == label)[0]\n",
        "  pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
        "  pyplot.legend()\n",
        "pyplot.show()\n"
      ],
      "metadata": {
        "id": "Ux8ihd_iZpF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "0wj5xmJSZr_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate a model using repeated k-fold cross-validation\n",
        "def evaluate_model(X, y, model):\n",
        "  # define the evaluation procedure\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  # evaluate the model on the dataset\n",
        "  scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1) # return scores from each fold and each repeat\n",
        "  return scores"
      ],
      "metadata": {
        "id": "8fUZsB1uZ5sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "scores = evaluate_model(X_test, y_test, model)\n",
        "# summarize performance\n",
        "print('Mean Accuracy: %.2f%%' % (mean(scores) * 100))"
      ],
      "metadata": {
        "id": "stp5pRs6Z8GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)\n",
        "y_pred=model.predict(X_test)\n",
        "pos_probs = y_pred\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix\\n')\n",
        "print(confusion)"
      ],
      "metadata": {
        "id": "I_KBE1ynZ_dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate roc auc\n",
        "roc_auc = roc_auc_score(y_test, pos_probs)\n",
        "print('No Skill ROC AUC %.3f' % roc_auc)"
      ],
      "metadata": {
        "id": "HJs-Wk1hboXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# plot no skill roc curve\n",
        "pyplot.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n",
        "# calculate roc curve for model\n",
        "fpr, tpr, _ = roc_curve(y_test, pos_probs)\n",
        "# plot model roc curve\n",
        "pyplot.plot(fpr, tpr, marker='.', label='Logistic')\n",
        "# axis labels\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "# show the legend\n",
        "pyplot.legend()\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "HOKoCvZGa0M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matthews_corrcoef(y_test,y_pred)"
      ],
      "metadata": {
        "id": "8pK41MIcb2m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test, y_pred, average='macro')"
      ],
      "metadata": {
        "id": "-sRFaxuFcO9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test, y_pred, average='micro')"
      ],
      "metadata": {
        "id": "C6fo8kKOcTaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test, y_pred, average='weighted')"
      ],
      "metadata": {
        "id": "-ocoy_CIcW_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = ['class 0', 'class 1']"
      ],
      "metadata": {
        "id": "BQ53tTXIdI7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "db4H4n7gdD2t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}