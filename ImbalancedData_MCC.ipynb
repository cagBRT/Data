{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMCmIuN1g54YlI+OyGMkNuZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Data/blob/main/ImbalancedData_MCC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matthews Correlation Coefficient(MCC)"
      ],
      "metadata": {
        "id": "yJO0p_sBd7Sy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matthew's correlation coefficient, also abbreviated as MCC was invented by Brian Matthews in 1975.<br>\n",
        "\n",
        "MCC is a statistical tool used for model evaluation. Its job is to gauge or measure the difference between the predicted values and actual values"
      ],
      "metadata": {
        "id": "nJaWvod-d8rH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66kcE_OWWca5"
      },
      "outputs": [],
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -s https://github.com/cagBRT/Data.git cloned-repo\n",
        "%cd cloned-repo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import unique\n",
        "from numpy import where\n",
        "import matplotlib as plt\n",
        "from matplotlib import pyplot\n",
        "from sklearn.datasets import make_blobs\n",
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "PUGGtFu_ZELk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "kt8PVHQbZTfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import balanced_accuracy_score"
      ],
      "metadata": {
        "id": "bS-_zza-9pZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold"
      ],
      "metadata": {
        "id": "Y1vx6hdkZziv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "t746D4E6ZXst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For measuring model performance there are four metrics in addition to ROC curves.<br>\n",
        "Each of the four can someimes be misleading. <br>\n",
        "Which is why we will discuss MCC"
      ],
      "metadata": {
        "id": "h_qRNgJ2_xqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at a small example"
      ],
      "metadata": {
        "id": "YLy7UJ0OCZXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"images/DogsCats1.png\")"
      ],
      "metadata": {
        "id": "A4DGxIqyEDU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The dataset: 21 dogs, 3 cats**<br>\n",
        "Precision is the proportion of true positives out of all detected positives, or simply TP/(TP+FP).<br>\n",
        "\n",
        "In this case, dog photos are the positive class, and 18 out of 21 photos that were classified as dogs actually contain dogs.<br>\n",
        "\n",
        "**The precision is 18/21=86%.** <br>\n",
        "The recall is the number of true positives that are correctly classified (TP/(TP+FN)). From the above matrix it is easy to see that there are 20 true positives, and 18 of them are successfully detected.<br>\n",
        "**Recall is 18/(18+2), or 90%.**<br>\n",
        "\n",
        "**The F1 score is 88%**"
      ],
      "metadata": {
        "id": "-xp56a9cGoWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision and recall consider one class , the positive class, to be the class we are interested in. <br>\n",
        "\n",
        "They use only three of the values in the confusion matrix: TP, FP, and FN. The 4th value — TN — is not used in these metrics.<br>\n",
        "\n",
        "\n",
        "You can put any value in the TN cell —0, 100, infinity — and the precision, recall and F1-score will not change.\n",
        "\n"
      ],
      "metadata": {
        "id": "rdvDJmpbH6Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"images/DogsCats2.png\")"
      ],
      "metadata": {
        "id": "OJYfwqeAIsHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, flip the confusion matrix. Let’s consider “cat” to be the positive class, i.e., the one we are interested in.<br>\n",
        "The dataset is still 21 dogs, 3 cats<br>\n",
        "\n",
        "**Precision is 33%**,<br>\n",
        "**Recall 25%**, <br>\n",
        "**F1-score is 29%**<br>\n",
        "\n",
        "There are 24 datapoints, and 19 were correctly classified.<br>\n",
        "Accuracy for dogs: 90%<br>\n",
        "Accuracy for cats: 25%\n",
        "\n"
      ],
      "metadata": {
        "id": "mbkzbx6mJ-xl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The change the metrics occurs because the classes are imbalanced. Depending which one is the positive class in an imbalanced dataset can greatly change the metrics.**"
      ],
      "metadata": {
        "id": "2s-RY2ovKFIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matthews Correlation Coefficient**\n",
        "*For binary classification, there is another solution:* <BR>\n",
        "- treat the true class and the predicted class as two (binary) variables,\n",
        "- compute their correlation coefficient. <br>\n",
        "\n",
        "**The higher the correlation between true and predicted values, the better the prediction.** <br>\n",
        "\n",
        "This is the phi-coefficient (φ), rechristened Matthews Correlation Coefficient (MCC) when applied to classifiers."
      ],
      "metadata": {
        "id": "Y8XqkrXXWdCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"images/MCCEQUATION.png\" , width=640)"
      ],
      "metadata": {
        "id": "1pKMg7rPM2Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MCC value is always between -1 and 1, with 0 meaning that the classifier is no better than a random flip of a fair coin**. <br>\n",
        "\n",
        "**MCC is also perfectly symmetric**, so no class is more important than the other; if you switch the positive and negative, you’ll still get the same value.\n",
        "\n",
        "**MCC takes into account all four values in the confusion matrix**, and a high value (close to 1) means that both classes are predicted well, even if one class is disproportionately under- (or over-) represented."
      ],
      "metadata": {
        "id": "ccov4SaDWh2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A larger dataset example"
      ],
      "metadata": {
        "id": "N0mIkHHrNmWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create an impbalanced dataset**"
      ],
      "metadata": {
        "id": "StYTn9Tfu-fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define dataset\n",
        "X1, y1 = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1,\n",
        "                           weights=[0.99,0.01], flip_y=0, random_state=4)\n",
        "\n",
        "y2=y1.copy()"
      ],
      "metadata": {
        "id": "YttM91oLZg3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(y1)):\n",
        "  if y1[i]==1:\n",
        "    y2[i]=0\n",
        "  elif y1[i]==0:\n",
        "    y2[i]=1"
      ],
      "metadata": {
        "id": "qVbLC1-I11XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y1"
      ],
      "metadata": {
        "id": "v2wXUIbU6Wve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y2"
      ],
      "metadata": {
        "id": "L8ZWaOBV6YJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize class distribution\n",
        "counter = Counter(y1)\n",
        "print(counter)\n",
        "print(counter.items())\n",
        "print(counter.keys())"
      ],
      "metadata": {
        "id": "dWyGlZEyZmfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize class distribution\n",
        "counter = Counter(y2)\n",
        "print(counter)\n",
        "print(counter.items())\n",
        "print(counter.keys())"
      ],
      "metadata": {
        "id": "6w_4HbEzwJWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scatter plot of examples by class label\n",
        "for label,  _ in counter.items():\n",
        "  row_ix = where(y1 == label)[0]\n",
        "  pyplot.scatter(X1[row_ix, 0], X1[row_ix, 1], label=str(label))\n",
        "  pyplot.legend()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "Ux8ihd_iZpF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scatter plot of examples by class label\n",
        "for label,  _ in counter.items():\n",
        "  row_ix = where(y2 == label)[0]\n",
        "  pyplot.scatter(X1[row_ix, 0], X1[row_ix, 1], label=str(label))\n",
        "  pyplot.legend()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "PE5XdeSywUCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split the data into train and test sets**"
      ],
      "metadata": {
        "id": "Q3e_f78hvFOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X1_train, X1_test, y1_train, y1_test = train_test_split(\n",
        "    X1, y1, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "0wj5xmJSZr_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
        "    X1, y2, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "iX5ymahewhUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the model**"
      ],
      "metadata": {
        "id": "wtkf9GirvJk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate a model using repeated k-fold cross-validation\n",
        "def evaluate_model(X, y, model, X_train, y_train):\n",
        "  # define the evaluation procedure\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  # evaluate the model on the dataset\n",
        "  scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1) # return scores from each fold and each repeat\n",
        "  return scores"
      ],
      "metadata": {
        "id": "8fUZsB1uZ5sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model1 = LogisticRegression(solver='lbfgs')\n",
        "scores1 = evaluate_model(X1_test, y1_test, model1, X1_train, y1_train)"
      ],
      "metadata": {
        "id": "stp5pRs6Z8GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model2 = LogisticRegression(solver='lbfgs')\n",
        "scores2 = evaluate_model(X2_test, y2_test, model2, X2_train, y2_train)"
      ],
      "metadata": {
        "id": "GBh_CcdbwqZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calcuate the metrics for the model**"
      ],
      "metadata": {
        "id": "dDfi85R4vYP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1.fit(X1_train, y1_train)\n",
        "y_pred1=model1.predict(X1_test)"
      ],
      "metadata": {
        "id": "yL3KtL21_3Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize performance\n",
        "print('Mean Accuracy: %.2f%%' % (mean(scores1) * 100))\n",
        "print('Balanced Accuracy: %.2f%%'% balanced_accuracy_score(y1_test, y_pred1))"
      ],
      "metadata": {
        "id": "6gMufB1fBXAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "ConfusionMatrixDisplay.from_predictions(y1_test, y_pred1)"
      ],
      "metadata": {
        "id": "ypMBGLEK_YCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.fit(X2_train, y2_train)\n",
        "y_pred2=model2.predict(X2_test)\n",
        "# summarize performance\n",
        "print('Mean Accuracy: %.2f%%' % (mean(scores2) * 100))\n",
        "print('Balanced Accuracy: %.2f%%'% balanced_accuracy_score(y2_test, y_pred2))"
      ],
      "metadata": {
        "id": "6I42D86K_iyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_predictions(y2_test, y_pred2)"
      ],
      "metadata": {
        "id": "bfS-OvIiBe0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_prob=model1.predict_proba(X1_test)\n",
        "pos_probs1=pos_prob[:,1]"
      ],
      "metadata": {
        "id": "jzIDqOX3TDB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate roc auc\n",
        "roc_auc1 = roc_auc_score(y1_test, pos_probs1)\n",
        "print('ROC AUC %.3f' % roc_auc1)"
      ],
      "metadata": {
        "id": "m6u2dJ5TxL0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_prob=model2.predict_proba(X2_test)\n",
        "pos_probs2=pos_prob[:,1]"
      ],
      "metadata": {
        "id": "QcBV5GDQUZpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate roc auc\n",
        "roc_auc2 = roc_auc_score(y2_test, pos_probs2)\n",
        "print('ROC AUC %.3f' % roc_auc2)"
      ],
      "metadata": {
        "id": "rshQwBLPBpzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot no skill roc curve\n",
        "pyplot.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n",
        "# calculate roc curve for model\n",
        "fpr, tpr, _ = roc_curve(y1_test, pos_probs1)\n",
        "# plot model roc curve\n",
        "pyplot.plot(fpr, tpr, marker='.', label='Logistic')\n",
        "# axis labels\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "# show the legend\n",
        "pyplot.legend()\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "HOKoCvZGa0M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot no skill roc curve\n",
        "pyplot.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n",
        "# calculate roc curve for model\n",
        "fpr, tpr, _ = roc_curve(y2_test, pos_probs2)\n",
        "# plot model roc curve\n",
        "pyplot.plot(fpr, tpr, marker='.', label='Logistic')\n",
        "# axis labels\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "# show the legend\n",
        "pyplot.legend()\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "ljAZG4G8xT3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = ['class 0', 'class 1']"
      ],
      "metadata": {
        "id": "BQ53tTXIdI7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y1_test, y_pred1, target_names=target_names))"
      ],
      "metadata": {
        "id": "db4H4n7gdD2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.metrics import classification_report_imbalanced\n",
        "print(classification_report_imbalanced(y2_test, y_pred2, target_names=target_names))\n",
        "#Text summary of the precision, recall, specificity, geometric mean, and index balanced accuracy"
      ],
      "metadata": {
        "id": "kltJ-nt7yIlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from imblearn.metrics import macro_averaged_mean_absolute_error\n",
        "\n",
        "y_true_balanced = [1, 1, 2, 2]\n",
        "y_true_imbalanced = [1, 2, 2, 2]\n",
        "y_pred = [1, 2, 1, 2]\n",
        "print(\"True balanced:\", mean_absolute_error(y_true_balanced, y_pred))\n",
        "print(\"Macro Avg Absolute error Balanced:\",macro_averaged_mean_absolute_error(y_true_balanced, y_pred))\n",
        "print(\"Absolute error:\", mean_absolute_error(y_true_imbalanced, y_pred))\n",
        "print(\"Macro Avg Absolute error:\",macro_averaged_mean_absolute_error(y_true_imbalanced, y_pred))"
      ],
      "metadata": {
        "id": "vLX7KCAKDGrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MCC 1:\",matthews_corrcoef(y1_test,y_pred1))\n",
        "print(\"MCC 2:\",matthews_corrcoef(y2_test,y_pred2))"
      ],
      "metadata": {
        "id": "8pK41MIcb2m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tThA26XTE88P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42\n"
      ],
      "metadata": {
        "id": "UzXBBrb2EVM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X, y = make_classification(\n",
        "    n_classes=3,\n",
        "    class_sep=2,\n",
        "    weights=[0.1, 0.9],\n",
        "    n_informative=10,\n",
        "    n_redundant=1,\n",
        "    flip_y=0,\n",
        "    n_features=20,\n",
        "    n_clusters_per_class=4,\n",
        "    n_samples=5000,\n",
        "    random_state=RANDOM_STATE,\n",
        ")"
      ],
      "metadata": {
        "id": "cRuydtj2EaCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, random_state=RANDOM_STATE\n",
        ")"
      ],
      "metadata": {
        "id": "ssoMHqkyEck2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "-qvkv3rREe36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    SMOTE(random_state=RANDOM_STATE),\n",
        "    LogisticRegression(max_iter=10_000, random_state=RANDOM_STATE),\n",
        ")"
      ],
      "metadata": {
        "id": "atTWWYmGEhGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "klAfOc9xEhIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The geometric mean corresponds to the square root of the product of the sensitivity and specificity. Combining the two metrics should account for the balancing of the dataset."
      ],
      "metadata": {
        "id": "_ek1QWunFAb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.metrics import geometric_mean_score\n",
        "print(f\"The geometric mean is {geometric_mean_score(y_test, y_pred):.3f}\")"
      ],
      "metadata": {
        "id": "7DQ-6jsXElyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The index balanced accuracy can transform any metric to be used in imbalanced learning problems."
      ],
      "metadata": {
        "id": "o66A1bDHFJ0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.metrics import make_index_balanced_accuracy\n",
        "\n",
        "alpha = 0.1\n",
        "geo_mean = make_index_balanced_accuracy(alpha=alpha, squared=True)(geometric_mean_score)\n",
        "\n",
        "print(\n",
        "    f\"The IBA using alpha={alpha} and the geometric mean: \"\n",
        "    f\"{geo_mean(y_test, y_pred):.3f}\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "CKlxn1KNEpAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.5\n",
        "geo_mean = make_index_balanced_accuracy(alpha=alpha, squared=True)(geometric_mean_score)\n",
        "\n",
        "print(\n",
        "    f\"The IBA using alpha={alpha} and the geometric mean: \"\n",
        "    f\"{geo_mean(y_test, y_pred):.3f}\"\n",
        ")"
      ],
      "metadata": {
        "id": "1DvW96upEs8L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}