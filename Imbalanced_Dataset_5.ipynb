{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Imbalanced Dataset 5.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOtzZ+JgVz39lIyF0J5Yuud",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Data/blob/main/Imbalanced_Dataset_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7eYVMv6mFEt"
      },
      "source": [
        "Classification predictive modeling involves predicting a class label for an example. On some problems, a crisp class label is not required, and instead a probability of class membership is preferred. The probability summarizes the likelihood (or uncertainty) of an example belonging to each class label. Probabilities are more nuanced and can be interpreted by a human operator or a system in decision making"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaFGLYkamTJr"
      },
      "source": [
        "the probability of an example belonging to class 1 to represent the probabilities for binary classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVjVVUG8meet"
      },
      "source": [
        "This focus on predicted probabilities may mean that the crisp class labels predicted by a model are ignored. This focus may mean that a model that predicts probabilities may appear to have terrible performance when evaluated according to its crisp class labels, such as using accuracy or a similar score. This is because although the predicted probabilities may show skill, they must be interpreted with a threshold prior to being converted into crisp class labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDzInkYymwPz"
      },
      "source": [
        "the focus on predicted probabilities may also require that the probabilities predicted by some nonlinear models to be calibrated prior to being used or evaluated. Some models will learn calibrated probabilities as part of the training process (e.g. logistic regression), but many will not and will require calibration (e.g. support vector machines, decision trees, and neural networks). We will cover the topic of calibrating predicted probabilities in Chapter 22. A given probability metric is typically calculated for each example, then averaged across all examples in the training dataset. There are two popular metrics for evaluating predicted probabilities; they are:\n",
        "􏰀 Log Loss\n",
        "􏰀 Brier Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEYz1p9DsyId"
      },
      "source": [
        "from numpy import unique\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import brier_score_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVodvPnrgc4z"
      },
      "source": [
        "# create an imbalanced dataset\n",
        "\n",
        "# generate 2 class dataset\n",
        "X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.99], flip_y=0,\n",
        "  random_state=1)\n",
        "# summarize dataset\n",
        "classes = unique(y)\n",
        "total = len(y)\n",
        "for c in classes:\n",
        "  n_examples = len(y[y==c])\n",
        "  percent = n_examples / total * 100\n",
        "  print('> Class=%d : %d/%d (%.1f%%)' % (c, n_examples, total, percent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM42axVrtHZj"
      },
      "source": [
        "Running the example reports the log loss for each naive strategy. As expected, predicting certainty for each class label is punished with large log loss scores, with the case of being certain for the minority class in all cases resulting in a much larger score. We can see that predicting the distribution of examples in the dataset as the baseline results in a better score than either of the other naive measures. This baseline represents the no skill classifier and log loss scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Z68Pc6tR7C"
      },
      "source": [
        "# **LogLoss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHh7Feb1gvEn"
      },
      "source": [
        "# log loss for naive probability predictions.\n",
        "\n",
        "# split into train/test sets with same class ratio\n",
        "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2, stratify=y)\n",
        "# no skill prediction 0\n",
        "probabilities = [[1, 0] for _ in range(len(testy))] \n",
        "avg_logloss = log_loss(testy, probabilities) \n",
        "print('P(class0=1): Log Loss=%.3f' % (avg_logloss)) # no skill prediction 1\n",
        "probabilities = [[0, 1] for _ in range(len(testy))] \n",
        "avg_logloss = log_loss(testy, probabilities) \n",
        "print('P(class1=1): Log Loss=%.3f' % (avg_logloss)) # baseline probabilities\n",
        "probabilities = [[0.99, 0.01] for _ in range(len(testy))] \n",
        "avg_logloss = log_loss(testy, probabilities) \n",
        "print('Baseline: Log Loss=%.3f' % (avg_logloss))\n",
        "# perfect probabilities\n",
        "avg_logloss = log_loss(testy, testy) \n",
        "print('Perfect: Log Loss=%.3f' % (avg_logloss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJoLFdc_tONf"
      },
      "source": [
        "# **Brier Score**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoPBodpjtjjB"
      },
      "source": [
        "The Brier score, named for Glenn Brier, calculates the mean squared error between predicted probabilities and the expected values. The score summarizes the magnitude of the error in the predicted probabilities and is designed for binary classification problems. It is focused on evaluating the probabilities for the positive class. Nevertheless, it can be adapted for problems with multiple classes. It is also an appropriate probabilistic metric for imbalanced classification problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efiXh1T-tpEj"
      },
      "source": [
        "The error score is always between 0.0 and 1.0, where a model with perfect skill has a score of 0.0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4QarqlTuHdm"
      },
      "source": [
        "Running the example, we can see the scores for the naive models and the baseline no skill classifier. As we might expect, we can see that predicting a 0.0 for all examples results in a low score, as the mean squared error between all 0.0 predictions and mostly 0 classes in the test set results in a small value. Conversely, the error between 1.0 predictions and mostly 0 class values results in a larger error score. Importantly, we can see that the default no skill classifier results in a lower score than predicting all 0.0 values. Again, this represents the baseline score, below which models will demonstrate skill."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6THOvNx5g6mL"
      },
      "source": [
        "# brier score for naive probability predictions.\n",
        "\n",
        "# no skill prediction 0\n",
        "probabilities = [0.0 for _ in range(len(testy))] \n",
        "avg_brier = brier_score_loss(testy, probabilities) \n",
        "print('P(class1=0): Brier Score=%.4f' % (avg_brier)) # no skill prediction 1\n",
        "probabilities = [1.0 for _ in range(len(testy))] \n",
        "avg_brier = brier_score_loss(testy, probabilities) \n",
        "print('P(class1=1): Brier Score=%.4f' % (avg_brier)) # baseline probabilities\n",
        "probabilities = [0.01 for _ in range(len(testy))] \n",
        "avg_brier = brier_score_loss(testy, probabilities) \n",
        "print('Baseline: Brier Score=%.4f' % (avg_brier)) # perfect probabilities\n",
        "avg_brier = brier_score_loss(testy, testy)\n",
        "print('Perfect: Brier Score=%.4f' % (avg_brier))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWMtE44VuaNl"
      },
      "source": [
        "A common practice is to transform the score using a reference score, such as the no skill classifier. This is called a Brier Skill Score, or BSS, and is calculated as follows:\n",
        "   BrierSkillScore = 1 −\n",
        "(BrierScore/BrierScoreref)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPZ6VFegumSI"
      },
      "source": [
        "a BSS of 0.0. This represents a no skill prediction. Values below this will be negative and represent worse than no skill. Values above 0.0 represent skillful predictions with a perfect prediction value of 1.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8tGf2k9hRml"
      },
      "source": [
        "# brier skill score for naive probability predictions.\n",
        "\n",
        "# calculate the brier skill score\n",
        "def brier_skill_score(y, yhat, brier_ref): # calculate the brier score\n",
        "  bs = brier_score_loss(y, yhat)\n",
        "  # calculate skill score\n",
        "  return 1.0 - (bs / brier_ref)\n",
        "# generate 2 class dataset\n",
        "X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.99], flip_y=0, random_state=1)\n",
        "# split into train/test sets with same class ratio\n",
        "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2, stratify=y)\n",
        "# calculate reference\n",
        "probabilities = [0.01 \n",
        "for _ in range(len(testy))] \n",
        "brier_ref = brier_score_loss(testy, probabilities) \n",
        "print('Reference: Brier Score=%.4f' % (brier_ref)) # no skill prediction 0\n",
        "probabilities = [0.0 for _ in range(len(testy))]\n",
        "bss = brier_skill_score(testy, probabilities, brier_ref) \n",
        "print('P(class1=0): BSS=%.4f' % (bss))\n",
        "# no skill prediction 1\n",
        "probabilities = [1.0 for _ in range(len(testy))]\n",
        "bss = brier_skill_score(testy, probabilities, brier_ref) \n",
        "print('P(class1=1): BSS=%.4f' % (bss))\n",
        "# baseline probabilities\n",
        "probabilities = [0.01 for _ in range(len(testy))]\n",
        "bss = brier_skill_score(testy, probabilities, brier_ref) \n",
        "print('Baseline: BSS=%.4f' % (bss))\n",
        "# perfect probabilities\n",
        "bss = brier_skill_score(testy, testy, brier_ref) \n",
        "print('Perfect: BSS=%.4f' % (bss))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}