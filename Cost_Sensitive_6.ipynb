{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cost Sensitive 6.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMKVGfmslFKfTXvnm98TV7w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Data/blob/main/Cost_Sensitive_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCKg3M8cdfNw"
      },
      "source": [
        "# **Cost Sensitive Gradient Boostiing with XGBoost**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBEqnkNHggHa"
      },
      "source": [
        "XGBoost is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm, which attempts to accurately predict a target variable by combining the estimates of a set of simpler, weaker models. (Amazon)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBlORGFOdoRu"
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHafvMfMdu4P"
      },
      "source": [
        "Create and imbalanced dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy7bSvKmdtyn"
      },
      "source": [
        "# define dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=7)\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4FQ40UXdF2K"
      },
      "source": [
        "for label, _ in counter.items():\n",
        "  row_ix = where(y == label)[0]\n",
        "  pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label)) \n",
        "  pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw9-hgG2edmG"
      },
      "source": [
        "**Create and train an XGB model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd4knq_MdQ8R"
      },
      "source": [
        "model = XGBClassifier()\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.5f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eW9YCyQh_8L"
      },
      "source": [
        "**Create and train an XGB model weighted for the class imbalance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lz89XxPi5np"
      },
      "source": [
        "XGBoost is trained to minimize a loss function and the gradient in gradient boosting refers to the steepness of this loss function, e.g. the amount of error. <br>\n",
        "\n",
        "A small gradient means a small error and, in turn, a small change to the model to correct the error. A large error gradient during training in turn results in a large correction.\n",
        "-  Small Gradient: Small error or correction to the model.\n",
        "- Large Gradient: Large error or correction to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3Krhs0hjQ6Y"
      },
      "source": [
        "The scale_po_weight hyperparameter is set to the value of 1.0 and has the effect of weighing the balance of positive examples, relative to negative examples when boosting decision trees. <br>\n",
        "\n",
        "For an imbalanced binary classification dataset, **the negative class refers to the majority class (class 0)** and **the positive class refers to the minority class (class 1).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ap3SBMIdjMc"
      },
      "source": [
        "# define model\n",
        "model = XGBClassifier(scale_pos_weight=99)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.5f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OgJ9cOzjzWQ"
      },
      "source": [
        "**Use a search grid to find the weights for training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "digUpUt8ju86"
      },
      "source": [
        "Create the model and define the search grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE8k-MSGjteO"
      },
      "source": [
        "model = XGBClassifier()\n",
        "# define grid\n",
        "weights = [1, 10, 25, 50, 75, 99, 100, 1000]\n",
        "param_grid = dict(scale_pos_weight=weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KYBJRyIdokf"
      },
      "source": [
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) # define grid search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n",
        "# execute the grid search\n",
        "grid_result = grid.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icbUwXeslb3z"
      },
      "source": [
        "Notice there is a performance improvement with the grid search method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FweNcOb1kAbS"
      },
      "source": [
        "# report the best configuration\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) # report all configurations\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SVLduq1lkE2"
      },
      "source": [
        "**Assignment<br>**\n",
        "Modify the dataset to figure out when to use each method of imbalanced datasets"
      ]
    }
  ]
}