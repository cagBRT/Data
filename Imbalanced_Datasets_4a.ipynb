{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Imbalanced Datasets 4a.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyODjqSomkTCeDQuJWlZ3sLK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Data/blob/main/Imbalanced_Datasets_4a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -l -s https://github.com/cagBRT/Machine-Learning.git cloned-repo\n",
        "%cd cloned-repo"
      ],
      "metadata": {
        "id": "oN_7_kLIgr2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "d8hRHf_dg1cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suO5huUEIISG"
      },
      "source": [
        "# **Receiver Operating Characteristic (ROC) Curves and AUC**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln5AZJjELAi7"
      },
      "source": [
        "Most imbalanced classification problems involve two classes:<br>\n",
        "- a negative case with the majority of examples <br>\n",
        "- a positive case with a minority of examples.\n",
        "\n",
        "Two diagnostic tools that help in the interpretation of binary (two-class) classification predictive models are:<br>\n",
        "\n",
        "- ROC Curves <br>\n",
        "- Precision-Recall curves."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we train a classification model, we get the probability of getting a result. In this case, our example will be the likelihood of repaying a loan."
      ],
      "metadata": {
        "id": "_J4gaOX2mbfi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctVXZHLfLcC-"
      },
      "source": [
        "ROC Curves and Precision-Recall Curves provide a diagnostic tool for binary classification models.<br>\n",
        "\n",
        "ROC AUC and Precision-Recall AUC provide scores that summarize the curves and can be used to compare classifiers.<br>\n",
        "\n",
        "ROC Curves and ROC AUC can be optimistic on severely imbalanced classification problems with few samples of the minority class."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"images/ClassModel1.png\", width=600)"
      ],
      "metadata": {
        "id": "AuYfq3X0mKpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The probabilities usually range between 0 and 1. The higher the value, the more likely the person is to repay a loan."
      ],
      "metadata": {
        "id": "bdFkn6ZDmmSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step is to find a threshold to classify the probabilities as “will repay” or “won’t repay”.<br>\n",
        "All predictions at or above this threshold, are classified as “will repay”<br>\n",
        "All predictions below this threshold, are classified as “won’t repay"
      ],
      "metadata": {
        "id": "8dB_xiYgmtUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of TP and TN**"
      ],
      "metadata": {
        "id": "i4fa48_lndwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"images/ClassModel2.png\", width=600)"
      ],
      "metadata": {
        "id": "v6zNKw9bnoGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The False Positve Rates and the True Positive Rates decrease as the threshold increases**"
      ],
      "metadata": {
        "id": "u4Ll1Et5oB0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"images/ClassModel3.png\")"
      ],
      "metadata": {
        "id": "Kiq9FPeAoRdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The ROC Curve**<br>\n",
        "\n",
        "To plot the ROC curve, we need to calculate the TPR and FPR for many different thresholds <br>\n",
        "\n",
        "For each threshold, we plot the FPR value in the x-axis and the TPR value in the y-axis. We then join the dots with a line.\n",
        "\n"
      ],
      "metadata": {
        "id": "PSCDgFm-fiVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"images/ROCCurve.png\")"
      ],
      "metadata": {
        "id": "Om14sQjXknJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"images/ROCTypicalGraph.png\")"
      ],
      "metadata": {
        "id": "Hcua7hpVpvO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The point on an ROC curve closest to (0.0,1.0) theoretically identifies the ideal classification threshold. <br>\n",
        "\n",
        "However, several other real-world issues influence the selection of the ideal classification threshold. For example, perhaps false negatives cause far more pain than false positives.\n",
        "<br>\n",
        "\n",
        "A numerical metric called AUC summarizes the ROC curve into a single floating-point value."
      ],
      "metadata": {
        "id": "1rF9Dy38qB53"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02PxciVEIe-s"
      },
      "source": [
        "These plots summarize the performance of *binary classification models* on the positive class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60_bQ5QOHAhM"
      },
      "source": [
        "# example of a precision-recall auc for a predictive model\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "#importing accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "TS0-xPtOaMhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ShJp7MlHYsY"
      },
      "source": [
        "# generate 2 class dataset\n",
        "X, y = make_classification(n_samples=10000, n_classes=2,n_features=4, random_state=1,weights=[0.99])\n",
        "# split into train/test sets\n",
        "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDjMcW8DHbfv"
      },
      "source": [
        "# fit a model\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "model.fit(trainX, trainy)\n",
        "# predict probabilities\n",
        "y_pred=model.predict(testX)\n",
        "\n",
        "yhat = model.predict_proba(testX)\n",
        "# retrieve just the probabilities for the positive class\n",
        "pos_probs = yhat[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion = confusion_matrix(testy, y_pred)\n",
        "print('Confusion Matrix\\n')\n",
        "print(confusion)"
      ],
      "metadata": {
        "id": "MgNHKt-uaKZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2qVLJrgKGb1"
      },
      "source": [
        "**Plot the ROC Curve**s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa-segTxI3fD"
      },
      "source": [
        "The upper left point iin the plot is a perfect skill. <br>\n",
        "\n",
        "If a model has no skill at class prediction, then its performance will be the diagona line from lwer left to upper right. <br>\n",
        "\n",
        "If the performance falls below the diagonal line, it is worse than a no skill model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T60Q8BzMHdjI"
      },
      "source": [
        "# plot no skill roc curve\n",
        "pyplot.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n",
        "# calculate roc curve for model\n",
        "fpr, tpr, _ = roc_curve(testy, pos_probs)\n",
        "# plot model roc curve\n",
        "pyplot.plot(fpr, tpr, marker='.', label='Logistic')\n",
        "# axis labels\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "# show the legend\n",
        "pyplot.legend()\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Area Under the ROC Curve"
      ],
      "metadata": {
        "id": "2UyVVRiJsObX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"images/AUC.png\")"
      ],
      "metadata": {
        "id": "aE-j-ENUssUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The area covered below the line is called “**Area Under the Curve (AUC)**”.\n",
        "\n",
        " This is used to evaluate the performance of a classification model. <br>\n",
        " The higher the AUC, the better the model is at distinguishing between classes."
      ],
      "metadata": {
        "id": "3m8t6beJk6P1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUC represents the probability that a random positive (green) example is positioned to the right of a random negative (red) example.\n",
        "\n",
        "AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.\n",
        "\n",
        "AUC is desirable for the following two reasons:\n",
        "\n",
        "**1.AUC is scale-invariant**. It measures how well predictions are ranked, rather than their absolute values.<br>\n",
        "\n",
        "**2. AUC is classification-threshold-invariant**. It measures the quality of the model's predictions irrespective of what classification threshold is chosen.\n",
        "However, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases:\n",
        "\n",
        "**Scale invariance is not always desirable**. For example, sometimes we really do need well calibrated probability outputs, and AUC won’t tell us about that.\n",
        "\n",
        "**Classification-threshold invariance is not always desirable**. In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn't a useful metric for this type of optimization."
      ],
      "metadata": {
        "id": "Yf_nn_l9sGOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to improve AUC?<br>\n",
        "In order to improve AUC, it is overall to improve the performance of the classifier. Several measures could be taken for experimentation. However, it will depend on the problem and the data to decide which measure will work.<br>\n",
        ">(1) Feature normalization and scaling. Basically, this is a method that improves the performance of the linear (logistic) model.<br>\n",
        "(2) Improve class imbalance. In classification problems, a bunch of them have imbalance classes. Setting class weights, or performing upward/downward sampling will help.<br>\n",
        "(3) Optimize other scores. Defining the right score for the problem, and optimize the score will help the prediction performance.<br>\n",
        "(4) Explore different models. Among the classification model, choose the model that has the best performance on the problem.<br>\n",
        "(5) Tune the parameter through grid search. Grid search is an automatic way to tune your parameter.<br>\n",
        "(6) Error analysis. Go back to check the false positive and false negative cases and find the reasons for this.<br>\n",
        "(7) Include more features or fewer features.<br>\n",
        "(8) There are also researches on optimizing AUC scores directly through investigating the relationship of AUC and error rate, or with the models, leading to a more straightforward but also more complicated analysis."
      ],
      "metadata": {
        "id": "bJVXy8eSwCHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "High AUC means your algorithm does a good job at *ranking* the test data, with most negative cases at one end of a scale and positive cases at the other."
      ],
      "metadata": {
        "id": "PcW2A0HGw_Qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\n"
      ],
      "metadata": {
        "id": "OMG5jK5lwhnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize how AUC is affected by different level of separation/discrimination, the distribution of probability for both the positive and negative classes are plotted below. When the overlap of two classes increases, the harder it gets to separate them and results in the decrease of AUC - random separation, at which AUC is equal to 0.5. <br>\n",
        "\n",
        "Interestingly, **the classifier can be a good one after reversing the predictions if the ROC curve lies in the right-bottom corner with AUC <=0.5.**\n",
        "\n"
      ],
      "metadata": {
        "id": "IwMN3_YEudBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://sinyi-chou.github.io/images/classification/prob_dist_animation.gif\n",
        "Image(open('prob_dist_animation.gif','rb').read(), width=600)"
      ],
      "metadata": {
        "id": "3EpajVCKuqjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"images/multiple_ROCs_plot.png\", width=600)"
      ],
      "metadata": {
        "id": "Cl2dX0VGv5WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUC is a threshold-free metrics capable of measuring the overall performance of binary classifier.\n",
        "\n",
        "AUC can only be used in binary classification. In multinomial classification, one-to-rest AUC would be an option using the average of each class.\n",
        "\n",
        "AUC is a good metric when the rank of output probabilities is of interest.\n",
        "\n",
        "**Although AUC is powerful, it is not a cure-all. AUC is not suitable for heavily imbalanced class distribution and when the goal is to have well-calibrated probabilities.**\n",
        "\n",
        "Models with maximized AUC treat the weight between positive and negative class equally."
      ],
      "metadata": {
        "id": "fI1zGYJZujTZ"
      }
    }
  ]
}