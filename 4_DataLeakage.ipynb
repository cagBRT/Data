{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4:DataLeakage.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOIOpYzbo6/XvLsNvpyjuuq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Data/blob/main/4_DataLeakage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh5oL6MJDNqC"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -s https://github.com/cagBRT/Data.git cloned-repo\n",
        "%cd cloned-repo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8Csv3pya6ck"
      },
      "source": [
        "In this notebook we will discuss data leakage that occurs when splitting the data into training and test sets. If the split happens after the data has been prepared, this will cause data leakage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zrDQ-JGt9Ca"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(\"splitting-data.png\" , width=640)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaYl5G2NZRBA"
      },
      "source": [
        "**Data Leakage can occur during the data preparation phase of machine learning**<br>\n",
        "How data preparation techniques are applied to data matters. <br>\n",
        "A common approach to data preparation:<br>\n",
        "\n",
        "1. Prepare Dataset<br>\n",
        "2. Split Data<br>\n",
        "3. Evaluate Models<br>\n",
        "\n",
        "Although common, it is dangerously incorrect in most cases. \n",
        "\n",
        "Applying data preparation techniques before splitting data for model evaluation can lead to data leakage and can result in an incorrect estimate of a modelâ€™s\n",
        "performance on the problem. <br>\n",
        "\n",
        "Data leakage refers to the problem where information about the\n",
        "test or validation dataset, is made available to the model in the training dataset.<br>\n",
        "This leakage is often small and subtle but can have a marked effect on\n",
        "performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE9RDm0jaVon"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(\"splt1.png\" , width=640)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDkdB_KpDVPF"
      },
      "source": [
        "**Import the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJtQXMTMA1Ub"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRGRV2vhGa8e"
      },
      "source": [
        "**The Naive Approach**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eTonRMeGdUt"
      },
      "source": [
        "# naive approach to normalizing the data before splitting the data and evaluating the model\n",
        "\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
        "random_state=7)\n",
        "# standardize the dataset\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# fit the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "# evaluate the model\n",
        "yhat = model.predict(X_test)\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.3f' % (accuracy*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXTuRA2Z9EHJ"
      },
      "source": [
        "**Recommended Approach**\n",
        "1. Split Data.\n",
        "2. Fit Data Preparation on Training Dataset.\n",
        "3. Apply Data Preparation to Train and Test Datasets.\n",
        "4. Evaluate Models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0LECqp_cotp"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(\"splt2.png\" , width=640)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UtHdrNEGDXY"
      },
      "source": [
        "# correct approach for normalizing the data after the data is \n",
        "#split before the model is evaluated\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n",
        "                           n_redundant=5,random_state=7)\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, \n",
        "                                                    random_state=1)\n",
        "# define the scaler\n",
        "scaler = MinMaxScaler()\n",
        "# fit on the training dataset\n",
        "scaler.fit(X_train)\n",
        "# scale the training dataset\n",
        "X_train = scaler.transform(X_train)\n",
        "# scale the test dataset\n",
        "X_test = scaler.transform(X_test)\n",
        "# fit the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "# evaluate the model\n",
        "yhat = model.predict(X_test)\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.3f' % (accuracy*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1sK6oBkHCok"
      },
      "source": [
        "In this case, data leakage led to a less accurate model. We would normally expect data leakage to lead to a more accurate model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clAjtSi2BcC0"
      },
      "source": [
        "**Assignment**<br>\n",
        "1. Change the synthetic dataset - add more features, more data points. \n",
        "Does the accuracy change between the two methods?\n",
        "2. Change the train-test split. Does the accuracy change between the two methods? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNG89hrIH3Qk"
      },
      "source": [
        "**Cross Validation Example**<br>\n",
        "Using a data pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv_utXZLIhce"
      },
      "source": [
        "**Naive metho**d<br>\n",
        "We know this method will produce an incorrect accuracy score because of the data leakage allowed during the data preparation procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVz_jCplIlLl"
      },
      "source": [
        "# naive data preparation for model evaluation with k-fold cross-validation\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5,random_state=7)\n",
        "# standardize the dataset\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "# define the model\n",
        "model = LogisticRegression()\n",
        "# define the evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate the model using cross-validation\n",
        "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46pwmCpqJLL-"
      },
      "source": [
        "**Recommended Method**<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al6gka0XHy0Z"
      },
      "source": [
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUGUdzSlBIZ9"
      },
      "source": [
        "Use a pipeline to prepare the data.<br>\n",
        "Pipelines are used to assemble several steps together, which means they can be crossvalidated together. <br>\n",
        "Pipelines also help avoid leaking the test set to the training set<br>\n",
        "\n",
        "[sklearn.pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n",
        "                           n_redundant=5,\n",
        "random_state=7)\n",
        "# define the pipeline\n",
        "steps = list()\n",
        "steps.append(('scaler', MinMaxScaler()))\n",
        "steps.append(('model', LogisticRegression()))\n",
        "pipeline = Pipeline(steps=steps)"
      ],
      "metadata": {
        "id": "j_tYEuSB1bqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42kIZZXEHpZT"
      },
      "source": [
        "# define the evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate the model using cross-validation\n",
        "scores = cross_val_score(pipeline, X, y, scoring='accuracy', \n",
        "                         cv=cv, n_jobs=-1)\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}