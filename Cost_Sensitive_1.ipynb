{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cost Sensitive 1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOKEDl2kzaZlAj1BbkFbLTu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Data/blob/main/Cost_Sensitive_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSiXpY8v2UGr"
      },
      "source": [
        "# **Cost Sensitive Machine Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPOiuU2X0syk"
      },
      "source": [
        "Machine learning errors have different costs. <br>\n",
        "**For example:** <br>\n",
        ">predicting someone has cancer when they don't is not nearly as costly as predicting someone does not have cancer when they do. <br><br>\n",
        "\n",
        "With imbalanced datasets different errors can have vastly different costs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fptT3p5-Kbw3"
      },
      "source": [
        "Types of cost:<br>\n",
        "- cost of misclassification errors<br>\n",
        "- cost of tests or evaluation<br>\n",
        "- cost of labeling<br>\n",
        "- cost of intervention <br>\n",
        "- cost of unwanted acchievements or outcomes\n",
        "- cost of computatiion \n",
        "- cost of data collection\n",
        "- cost if human-computer interaction\n",
        "- cost of instability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRtFgoFJ1MPQ"
      },
      "source": [
        "Cost-sensitive techniques can be broken into three types:<br>\n",
        "- data sampling\n",
        "- algorithm modifications\n",
        "- ensemble methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLgXN9zp3CRz"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtJ9jpGU21nm"
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "from numpy import mean\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils.class_weight import compute_class_weight\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGsuFJAv3Fly"
      },
      "source": [
        "**Create an imbalanced dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkoGfRom3L1O"
      },
      "source": [
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=2)\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKWCGzwrWqf-"
      },
      "source": [
        "# scatter plot of examples by class label \n",
        "for label, _ in counter.items():\n",
        "  row_ix = where(y == label)[0]\n",
        "  pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label)) \n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd47BF2WJ_KY"
      },
      "source": [
        "**Create a logistic regresssion model**<br>\n",
        "**Use Cross Validation to evaluate the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQjQiEY0XAao"
      },
      "source": [
        "# define model\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) \n",
        "\n",
        "# summarize performance\n",
        "print('Mean ROC AUC: %.3f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp9Z-3nIMzjM"
      },
      "source": [
        "# **Weighted Logistic Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71XE9mCyM388"
      },
      "source": [
        "Use weighted logistic regression on an imbalanced dataset. <br>\n",
        "Each class label is given a weight for calculating cost. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrNXKpWmNag8"
      },
      "source": [
        "Weights are a hyperparameter that can be found:<br>\n",
        "- using a hyperparameter search\n",
        "- using an SME to set the cost\n",
        "- settiing and using a best practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfsXYRkeN_Cq"
      },
      "source": [
        "Best practice:<br>\n",
        "Use inverse class distribution for the weights<br>\n",
        "In this example, the difference between the classes is 100 to 1, so we set the weights as 1 to 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmoBOxD3XH38"
      },
      "source": [
        "# define model\n",
        "weights = {0:0.01, 1:1.0}\n",
        "model = LogisticRegression(solver='lbfgs', class_weight=weights)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.3f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUsUDUQTeFVx"
      },
      "source": [
        "**Use compute_class_weight to get the weights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xYi0-Y6XKmE"
      },
      "source": [
        "# calculate class weighting\n",
        "weighting = compute_class_weight('balanced', [0,1], y) \n",
        "print(weighting)\n",
        "\n",
        "#.5 to 50 == 1 to 100 == .01 to 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxGmJleaevXC"
      },
      "source": [
        "**Can use class_weight='balanced' to balance the weights **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voABIOpMXQqD"
      },
      "source": [
        "# define model\n",
        "model = LogisticRegression(solver='lbfgs', class_weight='balanced')\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.3f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oac-RdP2XWUQ"
      },
      "source": [
        "# grid search class weights with logistic regression for imbalanced classification\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=2)\n",
        "# define model\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "# define grid\n",
        "balance = [{0:100,1:1}, {0:10,1:1}, {0:1,1:1}, {0:1,1:10}, {0:1,1:100}] \n",
        "param_grid = dict(class_weight=balance)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# define grid search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv,\n",
        "scoring='roc_auc')\n",
        "# execute the grid search\n",
        "grid_result = grid.fit(X, y)\n",
        "# report the best configuration\n",
        "print('Best: %f using %s' % (grid_result.best_score_, grid_result.best_params_)) # report all configurations\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('%f (%f) with: %r' % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuhlzRW2XWaE"
      },
      "source": [
        "# Generate and plot a synthetic imbalanced classification dataset\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=3)\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "# scatter plot of examples by class label \n",
        "for label, _ in counter.items():\n",
        "  row_ix = where(y == label)[0]\n",
        "  pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label)) \n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw1gOLceaDo1"
      },
      "source": [
        "# fit a decision tree on an imbalanced classification dataset\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=3)\n",
        "# define model\n",
        "model = DecisionTreeClassifier()\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.3f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-8ekQ0EaK9e"
      },
      "source": [
        "# decision tree with class weight on an imbalanced classification dataset\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=3)\n",
        "# define model\n",
        "model = DecisionTreeClassifier(class_weight='balanced')\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.3f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "donoHW1xaPiH"
      },
      "source": [
        "\n",
        "# grid search class weights with decision tree for imbalance classification\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=3)\n",
        "# define model\n",
        "model = DecisionTreeClassifier()\n",
        "# define grid\n",
        "balance = [{0:100,1:1}, {0:10,1:1}, {0:1,1:1}, {0:1,1:10}, {0:1,1:100}] \n",
        "param_grid = dict(class_weight=balance)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
        "# define grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n",
        "# execute the grid search\n",
        "grid_result = grid.fit(X, y)\n",
        "# report the best configuration\n",
        "print('Best: %f using %s' % (grid_result.best_score_, grid_result.best_params_)) # report all configurations\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('%f (%f) with: %r' % (mean, stdev, param)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWnbNQI8aK_H"
      },
      "source": [
        "# Generate and plot a synthetic imbalanced classification dataset\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "# scatter plot of examples by class label \n",
        "for label, _ in counter.items():\n",
        "  row_ix = where(y == label)[0]\n",
        "  pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label)) \n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyT41qrqax6S"
      },
      "source": [
        "# fit a svm on an imbalanced classification dataset\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n",
        "# define model\n",
        "model = SVC(gamma='scale')\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.3f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm52YpSha4yi"
      },
      "source": [
        "# svm with class weight on an imbalanced classification dataset\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n",
        "# define model\n",
        "model = SVC(gamma='scale', class_weight='balanced')\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.3f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvuWdeT7a9Py"
      },
      "source": [
        "\n",
        "# grid search class weights with svm for imbalance classification\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n",
        "# define model\n",
        "model = SVC(gamma='scale')\n",
        "# define grid\n",
        "balance = [{0:100,1:1}, {0:10,1:1}, {0:1,1:1}, {0:1,1:10}, {0:1,1:100}] \n",
        "param_grid = dict(class_weight=balance)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "#define grid search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n",
        "# execute the grid search\n",
        "grid_result = grid.fit(X, y)\n",
        "# report the best configuration\n",
        "print('Best: %f using %s' % (grid_result.best_score_, grid_result.best_params_)) # report all configurations\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('%f (%f) with: %r' % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10pqTpMPbPyY"
      },
      "source": [
        "# Generate and plot a synthetic imbalanced classification dataset\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=4)\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "# scatter plot of examples by class label \n",
        "for label, _ in counter.items():\n",
        "  row_ix = where(y == label)[0]\n",
        "  pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label)) \n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oFWc2hbboyj"
      },
      "source": [
        "# standard neural network on an imbalanced classification dataset\n",
        "from sklearn.datasets import make_classification \n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "# prepare train and test dataset\n",
        "def prepare_data():\n",
        "  # generate 2d classification dataset\n",
        "  X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "  n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=4)\n",
        "    # split into train and test\n",
        "  n_train = 5000\n",
        "  trainX, testX = X[:n_train, :], X[n_train:, :] \n",
        "  trainy, testy = y[:n_train], y[n_train:] \n",
        "  return trainX, trainy, testX, testy\n",
        "# define the neural network model\n",
        "def define_model(n_input):\n",
        "# define model\n",
        "  model = Sequential()\n",
        "  # define first hidden layer and visible layer \n",
        "  model.add(Dense(10, input_dim=n_input, activation='relu',kernel_initializer='he_uniform'))\n",
        "  # define output layer\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # define loss and optimizer \n",
        "  model.compile(loss='binary_crossentropy', optimizer='sgd') \n",
        "  return model\n",
        "\n",
        "# prepare dataset\n",
        "trainX, trainy, testX, testy = prepare_data()\n",
        "# define the model\n",
        "n_input = trainX.shape[1]\n",
        "model = define_model(n_input)\n",
        "# fit model\n",
        "model.fit(trainX, trainy, epochs=100, verbose=0) \n",
        "# make predictions on the test dataset\n",
        "yhat = model.predict(testX)\n",
        "# evaluate the ROC AUC of the predictions\n",
        "score = roc_auc_score(testy, yhat)\n",
        "print('ROC AUC: %.3f' % score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5RGxB7YcjiY"
      },
      "source": [
        "\n",
        "# class weighted neural network on an imbalanced classification dataset\n",
        "from sklearn.datasets import make_classification \n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "# prepare train and test dataset\n",
        "def prepare_data():\n",
        "  # generate 2d classification dataset\n",
        "  X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "  n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=4)\n",
        "    # split into train and test\n",
        "  n_train = 5000\n",
        "  trainX, testX = X[:n_train, :], X[n_train:, :] \n",
        "  trainy, testy = y[:n_train], y[n_train:] \n",
        "  return trainX, trainy, testX, testy\n",
        "\n",
        "# define the neural network model\n",
        "def define_model(n_input):\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  # define first hidden layer and visible layer \n",
        "  model.add(Dense(10, input_dim=n_input, activation='relu',\n",
        "  kernel_initializer='he_uniform'))\n",
        "  # define output layer\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # define loss and optimizer \n",
        "  model.compile(loss='binary_crossentropy', optimizer='sgd') \n",
        "  return model\n",
        "\n",
        "  # prepare dataset\n",
        "trainX, trainy, testX, testy = prepare_data()\n",
        "# get the model\n",
        "n_input = trainX.shape[1]\n",
        "model = define_model(n_input)\n",
        "# fit model\n",
        "weights = {0:1, 1:100}\n",
        "history = model.fit(trainX, trainy, class_weight=weights, epochs=100, verbose=0) # evaluate model\n",
        "yhat = model.predict(testX)\n",
        "score = roc_auc_score(testy, yhat)\n",
        "print('ROC AUC: %.3f' % score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4FQ40UXdF2K"
      },
      "source": [
        "# Generate and plot a synthetic imbalanced classification dataset\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from matplotlib import pyplot\n",
        "from numpy import where\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=7)\n",
        "# summarize class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "# scatter plot of examples by class label \n",
        "for label, _ in counter.items():\n",
        "  row_ix = where(y == label)[0]\n",
        "  pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label)) \n",
        "  pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd4knq_MdQ8R"
      },
      "source": [
        "# fit xgboost on an imbalanced classification dataset\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=7)\n",
        "# define model\n",
        "model = XGBClassifier()\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.5f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KjpPvCEdWUt"
      },
      "source": [
        "# estimate a value for the scale_pos_weight xgboost hyperparameter\n",
        "from sklearn.datasets import make_classification \n",
        "from collections import Counter\n",
        "\n",
        "#generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=7)\n",
        "# count examples in each class\n",
        "counter = Counter(y)\n",
        "# estimate scale_pos_weight value\n",
        "estimate = counter[0] / counter[1] \n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ap3SBMIdjMc"
      },
      "source": [
        "# fit balanced xgboost on an imbalanced classification dataset\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=7)\n",
        "# define model\n",
        "model = XGBClassifier(scale_pos_weight=99)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1) # summarize performance\n",
        "print('Mean ROC AUC: %.5f' % mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KYBJRyIdokf"
      },
      "source": [
        "# grid search positive class weights with xgboost for imbalance classification\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "# generate dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
        "n_clusters_per_class=2, weights=[0.99], flip_y=0, random_state=7)\n",
        "# define model\n",
        "model = XGBClassifier()\n",
        "# define grid\n",
        "weights = [1, 10, 25, 50, 75, 99, 100, 1000]\n",
        "param_grid = dict(scale_pos_weight=weights)\n",
        "# define evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) # define grid search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n",
        "# execute the grid search\n",
        "grid_result = grid.fit(X, y)\n",
        "# report the best configuration\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) # report all configurations\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}